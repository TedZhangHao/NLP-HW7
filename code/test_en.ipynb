{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This file illustrates how you might experiment with the HMM interface.\n",
    "You can paste these commands in at the Python prompt, or execute `test_en.py` directly.\n",
    "A notebook interface is nicer than the plain Python prompt, so we provide\n",
    "a notebook version of this file as `test_en.ipynb`, which you can open with\n",
    "`jupyter` or with Visual Studio `code` (run it with the `nlp-class` kernel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import math\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from corpus import TaggedCorpus\n",
    "from eval import eval_tagging, model_cross_entropy, viterbi_error_rate\n",
    "from hmm import HiddenMarkovModel\n",
    "from crf_neural import ConditionalRandomFieldNeural as ConditionalRandomField\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up logging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.root.setLevel(level=logging.INFO)\n",
    "log = logging.getLogger(\"test_en\")       # For usage, see findsim.py in earlier assignment.\n",
    "logging.basicConfig(format=\"%(levelname)s : %(message)s\", level=logging.INFO)  # could change INFO to DEBUG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Switch working directory to the directory where the data live.  You may need to edit this line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Read 191873 tokens from ensup, enraw\n",
      "INFO : Created 26 tag types\n",
      "INFO : Created 18461 word types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(entrain)=8064  len(ensup)=4051  len(endev)=996\n"
     ]
    }
   ],
   "source": [
    "entrain = TaggedCorpus(Path(\"ensup\"), Path(\"enraw\"))                               # all training\n",
    "ensup =   TaggedCorpus(Path(\"ensup\"), tagset=entrain.tagset, vocab=entrain.vocab)  # supervised training\n",
    "endev =   TaggedCorpus(Path(\"endev\"), tagset=entrain.tagset, vocab=entrain.vocab)  # evaluation\n",
    "enraw =   TaggedCorpus(Path(\"enraw\"), tagset=entrain.tagset, vocab=entrain.vocab) \n",
    "print(f\"{len(entrain)=}  {len(ensup)=}  {len(endev)=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Read 95936 tokens from ensup\n",
      "INFO : Created 26 tag types\n",
      "INFO : Created 12466 word types\n",
      "INFO : Tagset: f['W', 'J', 'N', 'C', 'V', 'I', 'D', ',', 'M', 'P', '.', 'E', 'R', '`', \"'\", 'T', '$', ':', '-', '#', 'S', 'F', 'U', 'L', '_EOS_TAG_', '_BOS_TAG_']\n"
     ]
    }
   ],
   "source": [
    "known_vocab = TaggedCorpus(Path(\"ensup\")).vocab    # words seen with supervised tags; used in evaluation\n",
    "log.info(f\"Tagset: f{list(entrain.tagset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make an HMM.  Let's do some pre-training to approximately maximize the\n",
    "regularized log-likelihood on supervised training data.  In other words, the\n",
    "probabilities at the M step will just be supervised count ratios.\n",
    "\n",
    "On each epoch, you will see two progress bars: first it collects counts from\n",
    "all the sentences (E step), and then after the M step, it evaluates the loss\n",
    "function, which is the (unregularized) cross-entropy on the training set.\n",
    "\n",
    "The parameters don't actually matter during the E step because there are no\n",
    "hidden tags to impute.  The first M step will jump right to the optimal\n",
    "solution.  The code will try a second epoch with the revised parameters, but\n",
    "the result will be identical, so it will detect convergence and stop.\n",
    "\n",
    "We arbitrarily choose λ=1 for our add-λ smoothing at the M step, but it would\n",
    "be better to search for the best value of this hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_sup_then_eval(model, train_corpus, eval_corpus, save_as, λ=1.0):\n",
    "    loss = lambda m: model_cross_entropy(m, eval_corpus=train_corpus)  # 监督期常用CE\n",
    "    model.train(corpus=train_corpus, loss=loss, λ=λ, save_path=save_as)\n",
    "    return model\n",
    "\n",
    "def train_semisup_stop_on_dev(model, train_corpus, save_as, λ=1.0):\n",
    "    # 半监督阶段：用dev上的Viterbi error作早停（题目建议）\n",
    "    loss = lambda m: viterbi_error_rate(m, eval_corpus=endev, known_vocab=known_vocab)\n",
    "    model.train(corpus=train_corpus, loss=loss, λ=λ, save_path=save_as)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 996/996 [00:03<00:00, 281.28it/s]\n",
      "INFO : Cross-entropy: 12.6494 nats (= perplexity 311591.795)\n",
      "100%|██████████| 996/996 [00:04<00:00, 213.83it/s]\n",
      "INFO : Tagging accuracy: all: 8.176%, known: 8.809%, seen: 4.882%, novel: 0.330%\n",
      "100%|██████████| 4013/4013 [00:33<00:00, 119.10it/s]\n",
      "100%|██████████| 996/996 [00:03<00:00, 292.27it/s]\n",
      "INFO : Cross-entropy: 10.8648 nats (= perplexity 52301.519)\n",
      "100%|██████████| 996/996 [00:04<00:00, 222.90it/s]\n",
      "INFO : Tagging accuracy: all: 9.332%, known: 9.761%, seen: 5.219%, novel: 4.756%\n",
      "100%|██████████| 4013/4013 [00:34<00:00, 116.56it/s]\n",
      "100%|██████████| 996/996 [00:03<00:00, 295.25it/s]\n",
      "INFO : Cross-entropy: 10.8643 nats (= perplexity 52278.880)\n",
      "100%|██████████| 996/996 [00:04<00:00, 223.48it/s]\n",
      "INFO : Tagging accuracy: all: 9.412%, known: 10.000%, seen: 4.377%, novel: 2.906%\n",
      "INFO : Saved model to hmm_raw_only.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<hmm.HiddenMarkovModel at 0x2d2d5808670>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm = HiddenMarkovModel(entrain.tagset, entrain.vocab)\n",
    "train_semisup_stop_on_dev(hmm, enraw, \"hmm_raw_only.pkl\", λ=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 996/996 [00:03<00:00, 289.14it/s]\n",
      "INFO : Cross-entropy: 12.6499 nats (= perplexity 311727.861)\n",
      "100%|██████████| 996/996 [00:04<00:00, 217.41it/s]\n",
      "INFO : Tagging accuracy: all: 4.280%, known: 4.253%, seen: 6.061%, novel: 3.963%\n",
      "100%|██████████| 8064/8064 [01:15<00:00, 107.47it/s]\n",
      "100%|██████████| 996/996 [00:03<00:00, 268.09it/s]\n",
      "INFO : Cross-entropy: 7.9108 nats (= perplexity 2726.691)\n",
      "100%|██████████| 996/996 [00:04<00:00, 212.08it/s]\n",
      "INFO : Tagging accuracy: all: 88.463%, known: 92.537%, seen: 46.633%, novel: 46.103%\n",
      "100%|██████████| 8064/8064 [01:13<00:00, 109.17it/s]\n",
      "100%|██████████| 996/996 [00:03<00:00, 274.09it/s]\n",
      "INFO : Cross-entropy: 7.4819 nats (= perplexity 1775.550)\n",
      "100%|██████████| 996/996 [00:04<00:00, 223.50it/s]\n",
      "INFO : Tagging accuracy: all: 87.110%, known: 91.452%, seen: 43.771%, novel: 41.480%\n",
      "INFO : Saved model to hmm_sup_raw.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<hmm.HiddenMarkovModel at 0x2d2d3b886d0>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm = HiddenMarkovModel(entrain.tagset, entrain.vocab)\n",
    "train_semisup_stop_on_dev(hmm, entrain, \"hmm_sup_raw.pkl\", λ=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 996/996 [00:03<00:00, 283.54it/s]\n",
      "INFO : Cross-entropy: 12.6506 nats (= perplexity 311959.728)\n",
      "100%|██████████| 996/996 [00:04<00:00, 223.24it/s]\n",
      "INFO : Tagging accuracy: all: 3.516%, known: 3.219%, seen: 3.535%, novel: 7.794%\n",
      "100%|██████████| 16166/16166 [02:32<00:00, 105.93it/s]\n",
      "100%|██████████| 996/996 [00:03<00:00, 299.40it/s]\n",
      "INFO : Cross-entropy: 7.3557 nats (= perplexity 1565.039)\n",
      "100%|██████████| 996/996 [00:04<00:00, 225.62it/s]\n",
      "INFO : Tagging accuracy: all: 90.609%, known: 95.312%, seen: 41.582%, novel: 42.008%\n",
      "100%|██████████| 16166/16166 [02:47<00:00, 96.29it/s] \n",
      "INFO : Saved model to hmm_supx3_raw-32332.pkl\n",
      "100%|██████████| 996/996 [00:04<00:00, 244.36it/s]\n",
      "INFO : Cross-entropy: 7.1453 nats (= perplexity 1268.093)\n",
      "100%|██████████| 996/996 [00:04<00:00, 215.21it/s]\n",
      "INFO : Tagging accuracy: all: 90.208%, known: 94.913%, seen: 42.929%, novel: 40.885%\n",
      "INFO : Saved model to hmm_supx3_raw.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<hmm.HiddenMarkovModel at 0x2d2d582bfd0>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ensupx3_enraw = TaggedCorpus(\n",
    "    Path(\"ensup\"), Path(\"ensup\"), Path(\"ensup\"), Path(\"enraw\"),\n",
    "    tagset=entrain.tagset, vocab=entrain.vocab\n",
    ")\n",
    "hmm = HiddenMarkovModel(entrain.tagset, entrain.vocab)\n",
    "train_semisup_stop_on_dev(hmm, ensupx3_enraw, \"hmm_supx3_raw.pkl\", λ=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 996/996 [00:04<00:00, 238.59it/s]\n",
      "INFO : Cross-entropy: 12.6493 nats (= perplexity 311550.766)\n",
      "100%|██████████| 996/996 [00:04<00:00, 207.90it/s]\n",
      "INFO : Tagging accuracy: all: 2.969%, known: 2.926%, seen: 3.535%, novel: 3.369%\n",
      "100%|██████████| 4013/4013 [00:38<00:00, 104.59it/s]\n",
      "100%|██████████| 996/996 [00:03<00:00, 257.70it/s]\n",
      "INFO : Cross-entropy: 10.8651 nats (= perplexity 52317.236)\n",
      "100%|██████████| 996/996 [00:04<00:00, 216.61it/s]\n",
      "INFO : Tagging accuracy: all: 1.866%, known: 1.863%, seen: 2.189%, novel: 1.783%\n",
      "INFO : Saved model to hmm_raw_1.pkl\n",
      "INFO : Loaded model from hmm_raw_1.pkl\n",
      "100%|██████████| 4051/4051 [00:20<00:00, 195.14it/s]\n",
      "INFO : Cross-entropy: 10.8774 nats (= perplexity 52967.296)\n",
      "100%|██████████| 4051/4051 [00:50<00:00, 80.64it/s] \n",
      "100%|██████████| 4051/4051 [00:21<00:00, 192.22it/s]\n",
      "INFO : Cross-entropy: 7.4505 nats (= perplexity 1720.767)\n",
      "100%|██████████| 4051/4051 [00:41<00:00, 97.35it/s] \n",
      "100%|██████████| 4051/4051 [00:17<00:00, 232.54it/s]\n",
      "INFO : Cross-entropy: 7.4505 nats (= perplexity 1720.761)\n",
      "INFO : Saved model to hmm_raw_then_sup.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<hmm.HiddenMarkovModel at 0x2d2d5c59d60>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm = HiddenMarkovModel(entrain.tagset, entrain.vocab)\n",
    "train_semisup_stop_on_dev(hmm, enraw, \"hmm_raw_1.pkl\", λ=1.0)\n",
    "\n",
    "hmm = HiddenMarkovModel.load(\"hmm_raw_1.pkl\")\n",
    "train_sup_then_eval(hmm, ensup, endev, \"hmm_raw_then_sup.pkl\", λ=1.0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4051/4051 [00:14<00:00, 287.23it/s]\n",
      "INFO : Cross-entropy: 12.6437 nats (= perplexity 309809.268)\n",
      "100%|██████████| 4051/4051 [00:39<00:00, 102.53it/s]\n",
      "100%|██████████| 4051/4051 [00:14<00:00, 277.12it/s]\n",
      "INFO : Cross-entropy: 7.4505 nats (= perplexity 1720.760)\n",
      "100%|██████████| 4051/4051 [00:38<00:00, 105.37it/s]\n",
      "100%|██████████| 4051/4051 [00:13<00:00, 307.93it/s]\n",
      "INFO : Cross-entropy: 7.4505 nats (= perplexity 1720.767)\n",
      "INFO : Saved model to hmm_sup.pkl\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<hmm.HiddenMarkovModel at 0x2d2d5d17550>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hmm = HiddenMarkovModel(entrain.tagset, entrain.vocab)\n",
    "train_sup_then_eval(hmm, ensup, endev, \"hmm_sup.pkl\", λ=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Loaded model from hmm_sup.pkl\n",
      "100%|██████████| 996/996 [00:04<00:00, 221.03it/s]\n",
      "INFO : Cross-entropy: 7.5995 nats (= perplexity 1997.182)\n",
      "100%|██████████| 996/996 [00:05<00:00, 180.25it/s]\n",
      "INFO : Tagging accuracy: all: 88.663%, known: 93.059%, seen: 44.108%, novel: 42.734%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.11336590254290368\n"
     ]
    }
   ],
   "source": [
    "hmm = HiddenMarkovModel.load(\"hmm_sup.pkl\")  # reset to supervised model (in case you're re-executing this bit)\n",
    "loss_dev = lambda model: viterbi_error_rate(model, eval_corpus=endev, \n",
    "                                            known_vocab=known_vocab)\n",
    "print(loss_dev(hmm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's throw in the unsupervised training data as well, and continue\n",
    "training as before, in order to increase the regularized log-likelihood on\n",
    "this larger, semi-supervised training set.  It's now the *incomplete-data*\n",
    "log-likelihood.\n",
    "\n",
    "This time, we'll use a different evaluation loss function: we'll stop when the\n",
    "*tagging error rate* on a held-out dev set stops getting better.  Also, the\n",
    "implementation of this loss function (`viterbi_error_rate`) includes a helpful\n",
    "side effect: it logs the *cross-entropy* on the held-out dataset as well, just\n",
    "for your information.\n",
    "\n",
    "We hope that held-out tagging accuracy will go up for a little bit before it\n",
    "goes down again (see Merialdo 1994). (Log-likelihood on training data will\n",
    "continue to improve, and that improvement may generalize to held-out\n",
    "cross-entropy.  But getting accuracy to increase is harder.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Read 95936 tokens from ensup\n",
      "INFO : Created 26 tag types\n",
      "INFO : Created 12466 word types\n",
      "INFO : Read 23949 tokens from endev\n",
      "INFO : Created 23 tag types\n",
      "INFO : Created 4959 word types\n",
      "100%|██████████| 4051/4051 [00:14<00:00, 276.47it/s]\n",
      "INFO : Cross-entropy: 12.2669 nats (= perplexity 212533.383)\n",
      "100%|██████████| 4051/4051 [00:22<00:00, 182.80it/s]\n",
      "INFO : Tagging accuracy: all: 3.749%, seen: 3.749%, novel: nan%\n",
      "100%|██████████| 4051/4051 [00:39<00:00, 101.32it/s]\n",
      "100%|██████████| 4051/4051 [00:14<00:00, 285.40it/s]\n",
      "INFO : Cross-entropy: 7.2282 nats (= perplexity 1377.693)\n",
      "100%|██████████| 4051/4051 [00:22<00:00, 180.52it/s]\n",
      "INFO : Tagging accuracy: all: 92.571%, seen: 92.571%, novel: nan%\n",
      "100%|██████████| 4051/4051 [00:41<00:00, 97.01it/s] \n",
      "100%|██████████| 4051/4051 [00:15<00:00, 267.01it/s]\n",
      "INFO : Cross-entropy: 7.2282 nats (= perplexity 1377.695)\n",
      "100%|██████████| 4051/4051 [00:22<00:00, 178.92it/s]\n",
      "INFO : Tagging accuracy: all: 92.571%, seen: 92.571%, novel: nan%\n",
      "INFO : Saved model to en_hmm_sup.pkl\n"
     ]
    }
   ],
   "source": [
    "ensup =   TaggedCorpus(Path(\"ensup\"))  # supervised training\n",
    "endev =   TaggedCorpus(Path(\"endev\"))  # supervised training\n",
    "hmm = HiddenMarkovModel(ensup.tagset, ensup.vocab)  \n",
    "loss_raw = lambda model: viterbi_error_rate(model, eval_corpus=ensup)\n",
    "hmm.train(corpus=ensup, loss=loss_raw, λ=1.0,\n",
    "          save_path=\"en_hmm_sup.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4013/4013 [00:13<00:00, 303.49it/s]\n",
      "INFO : Cross-entropy: 9.5973 nats (= perplexity 14724.539)\n",
      "100%|██████████| 4013/4013 [00:35<00:00, 114.31it/s]\n",
      "100%|██████████| 4013/4013 [00:12<00:00, 327.13it/s]\n",
      "INFO : Cross-entropy: 7.7891 nats (= perplexity 2414.232)\n",
      "100%|██████████| 4013/4013 [00:35<00:00, 114.62it/s]\n",
      "100%|██████████| 4013/4013 [00:12<00:00, 331.13it/s]\n",
      "INFO : Cross-entropy: 7.7891 nats (= perplexity 2414.222)\n",
      "INFO : Saved model to en_hmm_raw_0.pkl\n"
     ]
    }
   ],
   "source": [
    "# hmm = HiddenMarkovModel.load(\"en_hmm_raw_2.pkl\")  # reset to supervised model (in case you're re-executing this bit)\n",
    "hmm = HiddenMarkovModel(enraw.tagset, enraw.vocab)  \n",
    "loss_raw = lambda model: model_cross_entropy(model, eval_corpus=enraw)\n",
    "hmm.train(corpus=enraw, loss=loss_raw, λ=1.0,\n",
    "          save_path=\"en_hmm_raw_0.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also retry the above workflow where you start with a worse supervised\n",
    "model (like Merialdo).  Does EM help more in that case?  It's easiest to rerun\n",
    "exactly the code above, but first make the `ensup` file smaller by copying\n",
    "`ensup-tiny` over it.  `ensup-tiny` is only 25 sentences (that happen to cover\n",
    "all tags in `endev`).  Back up your old `ensup` and your old `*.pkl` models\n",
    "before you do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More detailed look at the first 10 sentences in the held-out corpus,\n",
    "including Viterbi tagging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_at_your_data(model, dev, N):\n",
    "    for m, sentence in enumerate(dev):\n",
    "        if m >= N: break\n",
    "        viterbi = model.viterbi_tagging(sentence.desupervise(), endev)\n",
    "        counts = eval_tagging(predicted=viterbi, gold=sentence, \n",
    "                              known_vocab=known_vocab)\n",
    "        num = counts['NUM', 'ALL']\n",
    "        denom = counts['DENOM', 'ALL']\n",
    "        \n",
    "        log.info(f\"Gold:    {sentence}\")\n",
    "        log.info(f\"Viterbi: {viterbi}\")\n",
    "        log.info(f\"Loss:    {denom - num}/{denom}\")\n",
    "        xent = -model.logprob(sentence, endev) / len(sentence)  # measured in nats\n",
    "        log.info(f\"Cross-entropy: {xent/math.log(2)} nats (= perplexity {math.exp(xent)})\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_at_your_data_compare(model, model_new, dev, N):\n",
    "    for m, sentence in enumerate(dev):\n",
    "        if m >= N: break\n",
    "        viterbi = model.viterbi_tagging(sentence.desupervise(), endev)\n",
    "        viterbi_new = model_new.viterbi_tagging(sentence.desupervise(), endev)\n",
    "        counts = eval_tagging(predicted=viterbi, gold=sentence, \n",
    "                              known_vocab=known_vocab)\n",
    "        counts_new = eval_tagging(predicted=viterbi_new, gold=sentence, \n",
    "                              known_vocab=known_vocab)\n",
    "        num = counts['NUM', 'ALL']\n",
    "        denom = counts['DENOM', 'ALL']\n",
    "        num_new = counts_new['NUM', 'ALL']\n",
    "        denom_new = counts_new['DENOM', 'ALL']\n",
    "        \n",
    "        log.info(f\"Gold:    {sentence}\")\n",
    "        log.info(f\"Supervised-Viterbi: {viterbi}\")\n",
    "        log.info(f\"Semi-Supervised-Viterbi: {viterbi_new}\")\n",
    "        log.info(f\"Supervised-Loss:    {denom - num}/{denom}\")\n",
    "        log.info(f\"Semi-Supervised-Loss:    {denom_new - num_new}/{denom_new}\")\n",
    "        xent = -model.logprob(sentence, endev) / len(sentence)  # measured in nats\n",
    "        xent_new = -model.logprob(sentence, endev) / len(sentence)  # measured in nats\n",
    "        log.info(f\"Supervised-Cross-entropy: {xent/math.log(2)} nats (= perplexity {math.exp(xent)})\\n---\")\n",
    "        log.info(f\"Semi-supervised-Cross-entropy: {xent_new/math.log(2)} nats (= perplexity {math.exp(xent)})\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Loaded model from en_hmm.pkl\n",
      "INFO : Gold:    ``/` We/P 're/V strongly/R _OOV_/V that/I anyone/N who/W has/V eaten/V in/I the/D cafeteria/N this/D month/N have/V the/D shot/N ,/, ''/' Mr./N Mattausch/N added/V ,/, ``/` and/C that/D means/V virtually/R everyone/N who/W works/V here/R ./.\n",
      "INFO : Supervised-Viterbi: ``/` We/P 're/V strongly/D _OOV_/N that/I anyone/N who/W has/V eaten/V in/I the/D cafeteria/I this/D month/N have/V the/D shot/N ,/, ''/' Mr./N Mattausch/, added/V ,/, ``/` and/C that/I means/V virtually/R everyone/, who/W works/V here/R ./.\n",
      "INFO : Semi-Supervised-Viterbi: ``/` We/P 're/V strongly/R _OOV_/V that/I anyone/N who/W has/V eaten/V in/I the/D cafeteria/N this/D month/N have/V the/D shot/N ,/, ''/' Mr./N Mattausch/T added/V ,/, ``/` and/C that/I means/V virtually/R everyone/, who/W works/V here/R ./.\n",
      "INFO : Supervised-Loss:    6/34\n",
      "INFO : Semi-Supervised-Loss:    3/34\n",
      "INFO : Supervised-Cross-entropy: 11.143198013305664 nats (= perplexity 2261.7089960764656)\n",
      "---\n",
      "INFO : Semi-supervised-Cross-entropy: 11.143198013305664 nats (= perplexity 2261.7089960764656)\n",
      "---\n",
      "INFO : Gold:    I/P was/V _OOV_/V to/T read/V the/D _OOV_/N of/I facts/N in/I your/P Oct./N 13/C editorial/N ``/` _OOV_/N 's/P _OOV_/N _OOV_/N ./. ''/'\n",
      "INFO : Supervised-Viterbi: I/P was/V _OOV_/V to/T read/V the/D _OOV_/N of/I facts/N in/I your/P Oct./N 13/C editorial/, ``/` _OOV_/P 's/V _OOV_/D _OOV_/N ./. ''/'\n",
      "INFO : Semi-Supervised-Viterbi: I/P was/V _OOV_/V to/T read/V the/D _OOV_/N of/I facts/N in/I your/P Oct./N 13/C editorial/, ``/` _OOV_/P 's/V _OOV_/D _OOV_/N ./. ''/'\n",
      "INFO : Supervised-Loss:    4/21\n",
      "INFO : Semi-Supervised-Loss:    4/21\n",
      "INFO : Supervised-Cross-entropy: 11.09935188293457 nats (= perplexity 2194.0068942991575)\n",
      "---\n",
      "INFO : Semi-supervised-Cross-entropy: 11.09935188293457 nats (= perplexity 2194.0068942991575)\n",
      "---\n",
      "INFO : Gold:    It/P is/V the/D _OOV_/J guerrillas/N who/W are/V aligned/V with/I the/D drug/N traffickers/N ,/, not/R the/D left/J _OOV_/N ./.\n",
      "INFO : Supervised-Viterbi: It/P is/V the/D _OOV_/N guerrillas/, who/W are/V aligned/V with/I the/D drug/N traffickers/N ,/, not/R the/D left/J _OOV_/N ./.\n",
      "INFO : Semi-Supervised-Viterbi: It/P is/V the/D _OOV_/N guerrillas/, who/W are/V aligned/R with/I the/D drug/N traffickers/N ,/, not/R the/D left/J _OOV_/N ./.\n",
      "INFO : Supervised-Loss:    2/18\n",
      "INFO : Semi-Supervised-Loss:    3/18\n",
      "INFO : Supervised-Cross-entropy: 9.995898246765137 nats (= perplexity 1021.0925338565584)\n",
      "---\n",
      "INFO : Semi-supervised-Cross-entropy: 9.995898246765137 nats (= perplexity 1021.0925338565584)\n",
      "---\n",
      "INFO : Gold:    This/D information/N was/V _OOV_/V from/I your/P own/J news/N stories/N on/I the/D region/N ./.\n",
      "INFO : Supervised-Viterbi: This/D information/N was/V _OOV_/V from/I your/P own/J news/N stories/N on/I the/D region/N ./.\n",
      "INFO : Semi-Supervised-Viterbi: This/D information/N was/V _OOV_/R from/I your/P own/J news/N stories/N on/I the/D region/N ./.\n",
      "INFO : Supervised-Loss:    0/13\n",
      "INFO : Semi-Supervised-Loss:    1/13\n",
      "INFO : Supervised-Cross-entropy: 9.696187973022461 nats (= perplexity 829.5515692654245)\n",
      "---\n",
      "INFO : Semi-supervised-Cross-entropy: 9.696187973022461 nats (= perplexity 829.5515692654245)\n",
      "---\n",
      "INFO : Gold:    _OOV_/J _OOV_/J government/N _OOV_/N of/I the/D ``/` _OOV_/F ''/' was/V due/J to/T the/D drug/N _OOV_/N '/P history/N of/I _OOV_/V out/R _OOV_/N in/I the/D _OOV_/N ./.\n",
      "INFO : Supervised-Viterbi: _OOV_/D _OOV_/J government/N _OOV_/N of/I the/D ``/` _OOV_/F ''/' was/V due/J to/T the/D drug/N _OOV_/I '/P history/N of/I _OOV_/N out/I _OOV_/N in/I the/D _OOV_/N ./.\n",
      "INFO : Semi-Supervised-Viterbi: _OOV_/D _OOV_/J government/N _OOV_/N of/I the/D ``/N _OOV_/, ''/' was/V due/J to/T the/D drug/N _OOV_/I '/P history/N of/I _OOV_/N out/I _OOV_/N in/I the/D _OOV_/N ./.\n",
      "INFO : Supervised-Loss:    4/25\n",
      "INFO : Semi-Supervised-Loss:    6/25\n",
      "INFO : Supervised-Cross-entropy: 11.262276649475098 nats (= perplexity 2456.308963987682)\n",
      "---\n",
      "INFO : Semi-supervised-Cross-entropy: 11.262276649475098 nats (= perplexity 2456.308963987682)\n",
      "---\n",
      "INFO : Gold:    Mary/N _OOV_/N Palo/N Alto/N ,/, Calif/N ./.\n",
      "INFO : Supervised-Viterbi: Mary/N _OOV_/I Palo/D Alto/N ,/, Calif/N ./.\n",
      "INFO : Semi-Supervised-Viterbi: Mary/N _OOV_/I Palo/D Alto/N ,/, Calif/N ./.\n",
      "INFO : Supervised-Loss:    2/7\n",
      "INFO : Semi-Supervised-Loss:    2/7\n",
      "INFO : Supervised-Cross-entropy: 10.41749095916748 nats (= perplexity 1367.6575791566843)\n",
      "---\n",
      "INFO : Semi-supervised-Cross-entropy: 10.41749095916748 nats (= perplexity 1367.6575791566843)\n",
      "---\n",
      "INFO : Gold:    I/P suggest/V that/I The/D Wall/N Street/N Journal/N -LRB-/- as/R well/R as/I other/J U.S./N news/N publications/N of/I like/J mind/N -RRB-/- should/M put/V its/P money/N where/W its/P mouth/N is/V :/: _OOV_/V computer/N equipment/N to/T replace/V that/I damaged/V at/I El/N _OOV_/N ,/, buy/V ad/N space/N ,/, publish/V stories/N under/I the/D _OOV_/N of/I El/N _OOV_/N journalists/N ./.\n",
      "INFO : Supervised-Viterbi: I/P suggest/V that/I The/D Wall/N Street/N Journal/N -LRB-/- as/I well/R as/I other/J U.S./N news/N publications/N of/I like/I mind/N -RRB-/- should/M put/V its/P money/N where/W its/P mouth/M is/V :/: _OOV_/D computer/N equipment/N to/T replace/V that/I damaged/N at/I El/D _OOV_/N ,/, buy/V ad/N space/N ,/, publish/V stories/N under/I the/D _OOV_/N of/I El/D _OOV_/J journalists/N ./.\n",
      "INFO : Semi-Supervised-Viterbi: I/P suggest/V that/I The/D Wall/N Street/N Journal/N -LRB-/- as/I well/R as/I other/J U.S./N news/N publications/N of/I like/I mind/N -RRB-/N should/M put/V its/P money/N where/W its/P mouth/M is/V :/I _OOV_/D computer/N equipment/N to/T replace/V that/I damaged/N at/I El/D _OOV_/N ,/, buy/V ad/N space/N ,/, publish/V stories/N under/I the/D _OOV_/N of/I El/D _OOV_/J journalists/N ./.\n",
      "INFO : Supervised-Loss:    8/53\n",
      "INFO : Semi-Supervised-Loss:    10/53\n",
      "INFO : Supervised-Cross-entropy: 11.922099113464355 nats (= perplexity 3880.6934845592136)\n",
      "---\n",
      "INFO : Semi-supervised-Cross-entropy: 11.922099113464355 nats (= perplexity 3880.6934845592136)\n",
      "---\n",
      "INFO : Gold:    Perhaps/R an/D arrangement/N could/M be/V worked/V out/R to/T ``/` sponsor/V ''/' El/N _OOV_/N journalists/N and/C staff/N by/I paying/V for/I added/V security/N in/I exchange/N for/I exclusive/J stories/N ./.\n",
      "INFO : Supervised-Viterbi: Perhaps/I an/D arrangement/N could/M be/V worked/V out/R to/T ``/` sponsor/N ''/' El/V _OOV_/T journalists/$ and/C staff/N by/I paying/V for/I added/D security/N in/I exchange/N for/I exclusive/D stories/N ./.\n",
      "INFO : Semi-Supervised-Viterbi: Perhaps/I an/D arrangement/N could/M be/V worked/V out/R to/T ``/` sponsor/F ''/' El/V _OOV_/T journalists/$ and/C staff/N by/I paying/V for/I added/J security/N in/I exchange/N for/I exclusive/J stories/N ./.\n",
      "INFO : Supervised-Loss:    7/27\n",
      "INFO : Semi-Supervised-Loss:    6/27\n",
      "INFO : Supervised-Cross-entropy: 11.79598617553711 nats (= perplexity 3555.8677606164765)\n",
      "---\n",
      "INFO : Semi-supervised-Cross-entropy: 11.79598617553711 nats (= perplexity 3555.8677606164765)\n",
      "---\n",
      "INFO : Gold:    _OOV_/V El/N _OOV_/N 's/P courage/N with/I real/J support/N ./.\n",
      "INFO : Supervised-Viterbi: _OOV_/D El/N _OOV_/I 's/P courage/N with/I real/J support/N ./.\n",
      "INFO : Semi-Supervised-Viterbi: _OOV_/D El/N _OOV_/I 's/P courage/N with/I real/J support/N ./.\n",
      "INFO : Supervised-Loss:    2/9\n",
      "INFO : Semi-Supervised-Loss:    2/9\n",
      "INFO : Supervised-Cross-entropy: 10.825379371643066 nats (= perplexity 1814.5287063519095)\n",
      "---\n",
      "INFO : Semi-supervised-Cross-entropy: 10.825379371643066 nats (= perplexity 1814.5287063519095)\n",
      "---\n",
      "INFO : Gold:    Douglas/N B./N Evans/N\n",
      "INFO : Supervised-Viterbi: Douglas/D B./N Evans/.\n",
      "INFO : Semi-Supervised-Viterbi: Douglas/D B./N Evans/.\n",
      "INFO : Supervised-Loss:    2/3\n",
      "INFO : Semi-Supervised-Loss:    2/3\n",
      "INFO : Supervised-Cross-entropy: 11.463462829589844 nats (= perplexity 2823.8798780720767)\n",
      "---\n",
      "INFO : Semi-supervised-Cross-entropy: 11.463462829589844 nats (= perplexity 2823.8798780720767)\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "hmm_sup = HiddenMarkovModel.load(\"en_hmm.pkl\")\n",
    "look_at_your_data_compare(hmm_sup, hmm, endev, N=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def look_at_your_data(model, dev, N):\n",
    "    for m, sentence in enumerate(dev):\n",
    "        if m >= N: break\n",
    "        viterbi = model.viterbi_tagging(sentence.desupervise(), endev)\n",
    "        counts = eval_tagging(predicted=viterbi, gold=sentence, \n",
    "                              known_vocab=known_vocab)\n",
    "        num = counts['NUM', 'ALL']\n",
    "        denom = counts['DENOM', 'ALL']\n",
    "        \n",
    "        log.info(f\"Gold:    {sentence}\")\n",
    "        log.info(f\"Viterbi: {viterbi}\")\n",
    "        log.info(f\"Loss:    {denom - num}/{denom}\")\n",
    "        xent = -model.logprob(sentence, endev) / len(sentence)  # measured in nats\n",
    "        log.info(f\"Cross-entropy: {xent/math.log(2)} nats (= perplexity {math.exp(xent)})\\n---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('_BOS_WORD_', '_BOS_TAG_'), ('``', '`'), ('We', 'P'), (\"'re\", 'V'), ('strongly', 'R'), ('_OOV_', 'V'), ('that', 'I'), ('anyone', 'N'), ('who', 'W'), ('has', 'V'), ('eaten', 'V'), ('in', 'I'), ('the', 'D'), ('cafeteria', 'N'), ('this', 'D'), ('month', 'N'), ('have', 'V'), ('the', 'D'), ('shot', 'N'), (',', ','), (\"''\", \"'\"), ('Mr.', 'N'), ('Mattausch', 'N'), ('added', 'V'), (',', ','), ('``', '`'), ('and', 'C'), ('that', 'D'), ('means', 'V'), ('virtually', 'R'), ('everyone', 'N'), ('who', 'W'), ('works', 'V'), ('here', 'R'), ('.', '.'), ('_EOS_WORD_', '_EOS_TAG_')]\n"
     ]
    }
   ],
   "source": [
    "for item in endev:\n",
    "    print(list(item))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_fixed_tokens(model1, model2, dev):\n",
    "    for m, sent in enumerate(dev):\n",
    "        pred1 = model1.viterbi_tagging(sent.desupervise(), endev)\n",
    "        pred2 = model2.viterbi_tagging(sent.desupervise(), endev)\n",
    "        i = 0\n",
    "        for (word, gold_tag) in sent:\n",
    "            # print(word)\n",
    "            # print(gold_tag, pred1[i], pred2[i])\n",
    "            if gold_tag != pred1[i][1] and gold_tag == pred2[i][1]:\n",
    "                print(f\"[FIXED] {word}: {pred1[i]} → {pred2[i]} (gold={gold_tag})\")\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[FIXED] strongly: ('strongly', 'D') → ('strongly', 'R') (gold=R)\n",
      "[FIXED] _OOV_: ('_OOV_', 'N') → ('_OOV_', 'V') (gold=V)\n",
      "[FIXED] cafeteria: ('cafeteria', 'I') → ('cafeteria', 'N') (gold=N)\n",
      "[FIXED] exclusive: ('exclusive', 'D') → ('exclusive', 'J') (gold=J)\n",
      "[FIXED] _OOV_: ('_OOV_', 'N') → ('_OOV_', 'V') (gold=V)\n",
      "[FIXED] up: ('up', 'I') → ('up', 'R') (gold=R)\n",
      "[FIXED] previously: ('previously', 'D') → ('previously', 'R') (gold=R)\n",
      "[FIXED] assumed: ('assumed', 'N') → ('assumed', 'V') (gold=V)\n",
      "[FIXED] offices: ('offices', 'V') → ('offices', 'N') (gold=N)\n",
      "[FIXED] packaging: ('packaging', 'D') → ('packaging', 'N') (gold=N)\n",
      "[FIXED] checks: ('checks', ',') → ('checks', 'N') (gold=N)\n",
      "[FIXED] checks: ('checks', 'P') → ('checks', 'N') (gold=N)\n",
      "[FIXED] 65: ('65', 'N') → ('65', 'C') (gold=C)\n",
      "[FIXED] _OOV_: ('_OOV_', 'V') → ('_OOV_', 'R') (gold=R)\n",
      "[FIXED] reckons: ('reckons', 'D') → ('reckons', 'V') (gold=V)\n",
      "[FIXED] out: ('out', 'I') → ('out', 'R') (gold=R)\n",
      "[FIXED] Latin: ('Latin', 'N') → ('Latin', 'J') (gold=J)\n",
      "[FIXED] _OOV_: ('_OOV_', 'N') → ('_OOV_', 'J') (gold=J)\n",
      "[FIXED] tells: ('tells', 'N') → ('tells', 'V') (gold=V)\n",
      "[FIXED] serious: ('serious', 'V') → ('serious', 'J') (gold=J)\n",
      "[FIXED] about: ('about', 'R') → ('about', 'I') (gold=I)\n",
      "[FIXED] 3.1: ('3.1', 'D') → ('3.1', 'C') (gold=C)\n",
      "[FIXED] back: ('back', 'R') → ('back', 'J') (gold=J)\n",
      "[FIXED] seat: ('seat', 'V') → ('seat', 'N') (gold=N)\n",
      "[FIXED] whose: ('whose', 'I') → ('whose', 'W') (gold=W)\n",
      "[FIXED] match: ('match', 'N') → ('match', 'V') (gold=V)\n",
      "[FIXED] vary: ('vary', 'N') → ('vary', 'V') (gold=V)\n",
      "[FIXED] advantages: ('advantages', 'V') → ('advantages', 'N') (gold=N)\n",
      "[FIXED] buys: ('buys', 'J') → ('buys', 'V') (gold=V)\n",
      "[FIXED] buys: ('buys', 'J') → ('buys', 'V') (gold=V)\n",
      "[FIXED] short-term: ('short-term', '$') → ('short-term', 'J') (gold=J)\n",
      "[FIXED] comfortable: ('comfortable', 'N') → ('comfortable', 'J') (gold=J)\n",
      "[FIXED] procedures: ('procedures', 'V') → ('procedures', 'N') (gold=N)\n",
      "[FIXED] Amex: ('Amex', 'C') → ('Amex', 'N') (gold=N)\n",
      "[FIXED] founded: ('founded', 'N') → ('founded', 'V') (gold=V)\n",
      "[FIXED] nothing: ('nothing', 'I') → ('nothing', 'N') (gold=N)\n",
      "[FIXED] Unfortunately: ('Unfortunately', 'N') → ('Unfortunately', 'R') (gold=R)\n",
      "[FIXED] Later: ('Later', 'N') → ('Later', 'R') (gold=R)\n",
      "[FIXED] Vietnam: ('Vietnam', 'V') → ('Vietnam', 'N') (gold=N)\n",
      "[FIXED] _OOV_: ('_OOV_', 'V') → ('_OOV_', 'R') (gold=R)\n",
      "[FIXED] sixth: ('sixth', 'N') → ('sixth', 'J') (gold=J)\n",
      "[FIXED] _OOV_: ('_OOV_', 'V') → ('_OOV_', 'N') (gold=N)\n",
      "[FIXED] newly: ('newly', 'N') → ('newly', 'R') (gold=R)\n",
      "[FIXED] _OOV_: ('_OOV_', 'N') → ('_OOV_', 'J') (gold=J)\n",
      "[FIXED] _OOV_: ('_OOV_', 'V') → ('_OOV_', 'N') (gold=N)\n",
      "[FIXED] province: ('province', ',') → ('province', 'N') (gold=N)\n",
      "[FIXED] entry: ('entry', 'V') → ('entry', 'N') (gold=N)\n",
      "[FIXED] Those: ('Those', 'N') → ('Those', 'D') (gold=D)\n",
      "[FIXED] Third: ('Third', 'N') → ('Third', 'J') (gold=J)\n",
      "[FIXED] Hunt: ('Hunt', 'J') → ('Hunt', 'N') (gold=N)\n",
      "[FIXED] silver: ('silver', 'J') → ('silver', 'N') (gold=N)\n",
      "[FIXED] Hunt: ('Hunt', 'W') → ('Hunt', 'N') (gold=N)\n",
      "[FIXED] asking: ('asking', 'I') → ('asking', 'V') (gold=V)\n",
      "[FIXED] money-market: ('money-market', 'N') → ('money-market', 'J') (gold=J)\n",
      "[FIXED] Malaysia: ('Malaysia', 'D') → ('Malaysia', 'N') (gold=N)\n",
      "[FIXED] _OOV_: ('_OOV_', 'N') → ('_OOV_', '$') (gold=$)\n",
      "[FIXED] _OOV_: ('_OOV_', 'N') → ('_OOV_', '$') (gold=$)\n",
      "[FIXED] abortion: ('abortion', 'P') → ('abortion', 'N') (gold=N)\n",
      "[FIXED] targets: ('targets', 'I') → ('targets', 'V') (gold=V)\n",
      "[FIXED] league: ('league', 'J') → ('league', 'N') (gold=N)\n",
      "[FIXED] more: ('more', 'J') → ('more', 'R') (gold=R)\n",
      "[FIXED] widens: ('widens', ',') → ('widens', 'V') (gold=V)\n",
      "[FIXED] Columbus: ('Columbus', 'V') → ('Columbus', 'N') (gold=N)\n",
      "[FIXED] Day: ('Day', 'T') → ('Day', 'N') (gold=N)\n",
      "[FIXED] oddly: ('oddly', 'V') → ('oddly', 'R') (gold=R)\n",
      "[FIXED] 52: ('52', 'N') → ('52', 'C') (gold=C)\n",
      "[FIXED] match: ('match', 'N') → ('match', 'V') (gold=V)\n",
      "[FIXED] unlikely: ('unlikely', 'N') → ('unlikely', 'J') (gold=J)\n",
      "[FIXED] behind: ('behind', 'N') → ('behind', 'R') (gold=R)\n",
      "[FIXED] 4.4: ('4.4', 'D') → ('4.4', 'C') (gold=C)\n",
      "[FIXED] fostered: ('fostered', 'I') → ('fostered', 'V') (gold=V)\n",
      "[FIXED] Fed: ('Fed', 'D') → ('Fed', 'N') (gold=N)\n",
      "[FIXED] centers: ('centers', ',') → ('centers', 'N') (gold=N)\n",
      "[FIXED] D'Arcy: (\"D'Arcy\", 'D') → (\"D'Arcy\", 'N') (gold=N)\n",
      "[FIXED] primarily: ('primarily', 'N') → ('primarily', 'R') (gold=R)\n",
      "[FIXED] sit: ('sit', 'N') → ('sit', 'V') (gold=V)\n",
      "[FIXED] _OOV_: ('_OOV_', '$') → ('_OOV_', 'C') (gold=C)\n",
      "[FIXED] Park: ('Park', 'V') → ('Park', 'N') (gold=N)\n",
      "[FIXED] fledgling: ('fledgling', 'D') → ('fledgling', 'N') (gold=N)\n",
      "[FIXED] water: ('water', 'D') → ('water', 'N') (gold=N)\n",
      "[FIXED] hit: ('hit', 'D') → ('hit', 'V') (gold=V)\n",
      "[FIXED] harder: ('harder', 'N') → ('harder', 'R') (gold=R)\n",
      "[FIXED] poor: ('poor', 'N') → ('poor', 'J') (gold=J)\n",
      "[FIXED] bid: ('bid', 'V') → ('bid', 'N') (gold=N)\n",
      "[FIXED] mark: ('mark', 'I') → ('mark', 'V') (gold=V)\n",
      "[FIXED] steel: ('steel', 'D') → ('steel', 'N') (gold=N)\n",
      "[FIXED] shipped: ('shipped', 'N') → ('shipped', 'V') (gold=V)\n",
      "[FIXED] steel: ('steel', 'D') → ('steel', 'N') (gold=N)\n",
      "[FIXED] structural: ('structural', 'C') → ('structural', 'J') (gold=J)\n",
      "[FIXED] steel: ('steel', 'D') → ('steel', 'N') (gold=N)\n",
      "[FIXED] steel: ('steel', 'D') → ('steel', 'N') (gold=N)\n",
      "[FIXED] damp: ('damp', '$') → ('damp', 'V') (gold=V)\n",
      "[FIXED] specialists: ('specialists', \"'\") → ('specialists', 'N') (gold=N)\n",
      "[FIXED] 13th: ('13th', 'N') → ('13th', 'J') (gold=J)\n",
      "[FIXED] that: ('that', 'I') → ('that', 'D') (gold=D)\n",
      "[FIXED] fear: ('fear', 'V') → ('fear', 'N') (gold=N)\n",
      "[FIXED] increases: ('increases', 'J') → ('increases', 'V') (gold=V)\n",
      "[FIXED] field: ('field', 'D') → ('field', 'N') (gold=N)\n",
      "[FIXED] smart: ('smart', 'V') → ('smart', 'J') (gold=J)\n",
      "[FIXED] leave: ('leave', 'N') → ('leave', 'V') (gold=V)\n",
      "[FIXED] _OOV_: ('_OOV_', 'N') → ('_OOV_', 'J') (gold=J)\n",
      "[FIXED] _OOV_: ('_OOV_', 'V') → ('_OOV_', 'N') (gold=N)\n",
      "[FIXED] 's: (\"'s\", 'P') → (\"'s\", 'V') (gold=V)\n",
      "[FIXED] Beach: ('Beach', ',') → ('Beach', 'N') (gold=N)\n",
      "[FIXED] Fed: ('Fed', 'D') → ('Fed', 'N') (gold=N)\n",
      "[FIXED] saw: ('saw', 'V') → ('saw', 'N') (gold=N)\n",
      "[FIXED] Unfortunately: ('Unfortunately', 'N') → ('Unfortunately', 'R') (gold=R)\n",
      "[FIXED] stock-market: ('stock-market', 'C') → ('stock-market', 'N') (gold=N)\n",
      "[FIXED] drugs: ('drugs', 'C') → ('drugs', 'N') (gold=N)\n",
      "[FIXED] junk-bond: ('junk-bond', 'N') → ('junk-bond', 'J') (gold=J)\n",
      "[FIXED] high-yield: ('high-yield', 'N') → ('high-yield', 'J') (gold=J)\n",
      "[FIXED] junk-bond: ('junk-bond', 'N') → ('junk-bond', 'J') (gold=J)\n",
      "[FIXED] junk-bond: ('junk-bond', 'N') → ('junk-bond', 'J') (gold=J)\n",
      "[FIXED] portfolios: ('portfolios', ',') → ('portfolios', 'N') (gold=N)\n",
      "[FIXED] Partnership: ('Partnership', 'I') → ('Partnership', 'N') (gold=N)\n",
      "[FIXED] 98: ('98', 'N') → ('98', 'C') (gold=C)\n",
      "[FIXED] out: ('out', 'I') → ('out', 'R') (gold=R)\n",
      "[FIXED] structured: ('structured', ',') → ('structured', 'V') (gold=V)\n",
      "[FIXED] 111: ('111', 'N') → ('111', 'C') (gold=C)\n"
     ]
    }
   ],
   "source": [
    "show_fixed_tokens(hmm_sup, hmm, endev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HiddenMarkovModel' object has no attribute 'A_contrib_log'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 15\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m (prev_tag, prev_word), val \u001b[38;5;129;01min\u001b[39;00m top:\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprev_tag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m→\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect_tag\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) in \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprev_word\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtarget_word\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m  contrib=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.3f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 15\u001b[0m \u001b[43mtrace_word_fix_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhmm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtells\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mV\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[39], line 5\u001b[0m, in \u001b[0;36mtrace_word_fix_context\u001b[1;34m(model, target_word, correct_tag, topk)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mtrace_word_fix_context\u001b[39m(model, target_word, correct_tag, topk\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m      3\u001b[0m     counter \u001b[38;5;241m=\u001b[39m collections\u001b[38;5;241m.\u001b[39mdefaultdict(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m prev_w, next_w, s, t, contrib \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mA_contrib_log\u001b[49m:\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;66;03m# 只看目标词的上下文\u001b[39;00m\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m next_w \u001b[38;5;241m==\u001b[39m target_word \u001b[38;5;129;01mand\u001b[39;00m model\u001b[38;5;241m.\u001b[39mtagset[t] \u001b[38;5;241m==\u001b[39m correct_tag:\n\u001b[0;32m      8\u001b[0m             counter[(model\u001b[38;5;241m.\u001b[39mtagset[s], prev_w)] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m contrib\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'HiddenMarkovModel' object has no attribute 'A_contrib_log'"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "def trace_word_fix_context(model, target_word, correct_tag, topk=10):\n",
    "    counter = collections.defaultdict(float)\n",
    "\n",
    "    for prev_w, next_w, s, t, contrib in model.A_contrib_log:\n",
    "        # 只看目标词的上下文\n",
    "        if next_w == target_word and model.tagset[t] == correct_tag:\n",
    "            counter[(model.tagset[s], prev_w)] += contrib\n",
    "\n",
    "    # 排序并输出前 topk 个最重要的上下文贡献\n",
    "    top = sorted(counter.items(), key=lambda kv: kv[1], reverse=True)[:topk]\n",
    "    print(f\"\\nContextual contributors for word '{target_word}' ({correct_tag}):\")\n",
    "    for (prev_tag, prev_word), val in top:\n",
    "        print(f\"  ({prev_tag}→{correct_tag}) in '{prev_word} {target_word}'  contrib={val:.3f}\")\n",
    "trace_word_fix_context(hmm, \"tells\", \"V\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Gold:    ``/` We/P 're/V strongly/R _OOV_/V that/I anyone/N who/W has/V eaten/V in/I the/D cafeteria/N this/D month/N have/V the/D shot/N ,/, ''/' Mr./N Mattausch/N added/V ,/, ``/` and/C that/D means/V virtually/R everyone/N who/W works/V here/R ./.\n",
      "INFO : Viterbi: ``/` We/P 're/V strongly/R _OOV_/V that/I anyone/N who/W has/V eaten/V in/I the/D cafeteria/N this/D month/N have/V the/D shot/N ,/, ''/' Mr./N Mattausch/T added/V ,/, ``/` and/C that/I means/V virtually/R everyone/, who/W works/V here/R ./.\n",
      "INFO : Loss:    3/34\n",
      "INFO : Cross-entropy: 10.617977142333984 nats (= perplexity 1571.554982254888)\n",
      "---\n",
      "INFO : Gold:    I/P was/V _OOV_/V to/T read/V the/D _OOV_/N of/I facts/N in/I your/P Oct./N 13/C editorial/N ``/` _OOV_/N 's/P _OOV_/N _OOV_/N ./. ''/'\n",
      "INFO : Viterbi: I/P was/V _OOV_/V to/T read/V the/D _OOV_/N of/I facts/N in/I your/P Oct./N 13/C editorial/, ``/` _OOV_/P 's/V _OOV_/D _OOV_/N ./. ''/'\n",
      "INFO : Loss:    4/21\n",
      "INFO : Cross-entropy: 10.876399040222168 nats (= perplexity 1879.846113700444)\n",
      "---\n",
      "INFO : Gold:    It/P is/V the/D _OOV_/J guerrillas/N who/W are/V aligned/V with/I the/D drug/N traffickers/N ,/, not/R the/D left/J _OOV_/N ./.\n",
      "INFO : Viterbi: It/P is/V the/D _OOV_/N guerrillas/, who/W are/V aligned/R with/I the/D drug/N traffickers/N ,/, not/R the/D left/J _OOV_/N ./.\n",
      "INFO : Loss:    3/18\n",
      "INFO : Cross-entropy: 9.65076732635498 nats (= perplexity 803.8414943595208)\n",
      "---\n",
      "INFO : Gold:    This/D information/N was/V _OOV_/V from/I your/P own/J news/N stories/N on/I the/D region/N ./.\n",
      "INFO : Viterbi: This/D information/N was/V _OOV_/R from/I your/P own/J news/N stories/N on/I the/D region/N ./.\n",
      "INFO : Loss:    1/13\n",
      "INFO : Cross-entropy: 9.34334945678711 nats (= perplexity 649.5737977294744)\n",
      "---\n",
      "INFO : Gold:    _OOV_/J _OOV_/J government/N _OOV_/N of/I the/D ``/` _OOV_/F ''/' was/V due/J to/T the/D drug/N _OOV_/N '/P history/N of/I _OOV_/V out/R _OOV_/N in/I the/D _OOV_/N ./.\n",
      "INFO : Viterbi: _OOV_/D _OOV_/J government/N _OOV_/N of/I the/D ``/N _OOV_/, ''/' was/V due/J to/T the/D drug/N _OOV_/I '/P history/N of/I _OOV_/N out/I _OOV_/N in/I the/D _OOV_/N ./.\n",
      "INFO : Loss:    6/25\n",
      "INFO : Cross-entropy: 10.976457595825195 nats (= perplexity 2014.85132166154)\n",
      "---\n",
      "INFO : Gold:    Mary/N _OOV_/N Palo/N Alto/N ,/, Calif/N ./.\n",
      "INFO : Viterbi: Mary/N _OOV_/I Palo/D Alto/N ,/, Calif/N ./.\n",
      "INFO : Loss:    2/7\n",
      "INFO : Cross-entropy: 10.526659965515137 nats (= perplexity 1475.1648150165854)\n",
      "---\n",
      "INFO : Gold:    I/P suggest/V that/I The/D Wall/N Street/N Journal/N -LRB-/- as/R well/R as/I other/J U.S./N news/N publications/N of/I like/J mind/N -RRB-/- should/M put/V its/P money/N where/W its/P mouth/N is/V :/: _OOV_/V computer/N equipment/N to/T replace/V that/I damaged/V at/I El/N _OOV_/N ,/, buy/V ad/N space/N ,/, publish/V stories/N under/I the/D _OOV_/N of/I El/N _OOV_/N journalists/N ./.\n",
      "INFO : Viterbi: I/P suggest/V that/I The/D Wall/N Street/N Journal/N -LRB-/- as/I well/R as/I other/J U.S./N news/N publications/N of/I like/I mind/N -RRB-/N should/M put/V its/P money/N where/W its/P mouth/M is/V :/I _OOV_/D computer/N equipment/N to/T replace/V that/I damaged/N at/I El/D _OOV_/N ,/, buy/V ad/N space/N ,/, publish/V stories/N under/I the/D _OOV_/N of/I El/D _OOV_/J journalists/N ./.\n",
      "INFO : Loss:    10/53\n",
      "INFO : Cross-entropy: 11.65014362335205 nats (= perplexity 3213.975672533252)\n",
      "---\n",
      "INFO : Gold:    Perhaps/R an/D arrangement/N could/M be/V worked/V out/R to/T ``/` sponsor/V ''/' El/N _OOV_/N journalists/N and/C staff/N by/I paying/V for/I added/V security/N in/I exchange/N for/I exclusive/J stories/N ./.\n",
      "INFO : Viterbi: Perhaps/I an/D arrangement/N could/M be/V worked/V out/R to/T ``/` sponsor/F ''/' El/V _OOV_/T journalists/$ and/C staff/N by/I paying/V for/I added/J security/N in/I exchange/N for/I exclusive/J stories/N ./.\n",
      "INFO : Loss:    6/27\n",
      "INFO : Cross-entropy: 11.456226348876953 nats (= perplexity 2809.749843531537)\n",
      "---\n",
      "INFO : Gold:    _OOV_/V El/N _OOV_/N 's/P courage/N with/I real/J support/N ./.\n",
      "INFO : Viterbi: _OOV_/D El/N _OOV_/I 's/P courage/N with/I real/J support/N ./.\n",
      "INFO : Loss:    2/9\n",
      "INFO : Cross-entropy: 10.914339065551758 nats (= perplexity 1929.9381681758487)\n",
      "---\n",
      "INFO : Gold:    Douglas/N B./N Evans/N\n",
      "INFO : Viterbi: Douglas/D B./N Evans/.\n",
      "INFO : Loss:    2/3\n",
      "INFO : Cross-entropy: 11.670413970947266 nats (= perplexity 3259.4523216532657)\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "look_at_your_data(hmm, endev, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try supervised training of a CRF (this doesn't use the unsupervised\n",
    "part of the data, so it is comparable to the supervised pre-training we did\n",
    "for the HMM).  We will use SGD to approximately maximize the regularized\n",
    "log-likelihood. \n",
    "\n",
    "As with the semi-supervised HMM training, we'll periodically evaluate the\n",
    "tagging accuracy (and also print the cross-entropy) on a held-out dev set.\n",
    "We use the default `eval_interval` and `tolerance`.  If you want to stop\n",
    "sooner, then you could increase the `tolerance` so the training method decides\n",
    "sooner that it has converged.\n",
    "\n",
    "We arbitrarily choose reg = 1.0 for L2 regularization, learning rate = 0.05,\n",
    "and a minibatch size of 10, but it would be better to search for the best\n",
    "value of these hyperparameters.\n",
    "\n",
    "Note that the logger reports the CRF's *conditional* cross-entropy, log p(tags\n",
    "| words) / n.  This is much lower than the HMM's *joint* cross-entropy log\n",
    "p(tags, words) / n, but that doesn't mean the CRF is worse at tagging.  The\n",
    "CRF is just predicting less information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : *** Conditional Random Field (CRF)\n",
      "\n",
      "100%|██████████| 996/996 [00:06<00:00, 160.34it/s]\n",
      "INFO : Cross-entropy: 3.0507 nats (= perplexity 21.131)\n",
      "100%|██████████| 996/996 [00:04<00:00, 222.31it/s]\n",
      "INFO : Tagging accuracy: all: 6.764%, known: 6.831%, seen: 4.209%, novel: 6.803%\n",
      "100%|██████████| 500/500 [00:09<00:00, 51.92it/s]\n",
      "100%|██████████| 996/996 [00:06<00:00, 155.69it/s]\n",
      "INFO : Cross-entropy: 0.9112 nats (= perplexity 2.487)\n",
      "100%|██████████| 996/996 [00:04<00:00, 227.68it/s]\n",
      "INFO : Tagging accuracy: all: 72.542%, known: 73.513%, seen: 58.754%, novel: 63.937%\n",
      "100%|██████████| 500/500 [00:09<00:00, 50.71it/s]\n",
      "100%|██████████| 996/996 [00:06<00:00, 152.10it/s]\n",
      "INFO : Cross-entropy: 0.7513 nats (= perplexity 2.120)\n",
      "100%|██████████| 996/996 [00:04<00:00, 217.26it/s]\n",
      "INFO : Tagging accuracy: all: 75.310%, known: 77.061%, seen: 55.892%, novel: 57.662%\n",
      "100%|██████████| 500/500 [00:09<00:00, 50.83it/s]\n",
      "100%|██████████| 996/996 [00:06<00:00, 164.51it/s]\n",
      "INFO : Cross-entropy: 0.6580 nats (= perplexity 1.931)\n",
      "100%|██████████| 996/996 [00:04<00:00, 236.61it/s]\n",
      "INFO : Tagging accuracy: all: 78.738%, known: 80.294%, seen: 61.785%, novel: 62.946%\n",
      "100%|██████████| 500/500 [00:09<00:00, 52.19it/s]\n",
      "100%|██████████| 996/996 [00:06<00:00, 155.95it/s]\n",
      "INFO : Cross-entropy: 0.6043 nats (= perplexity 1.830)\n",
      "100%|██████████| 996/996 [00:04<00:00, 220.44it/s]\n",
      "INFO : Tagging accuracy: all: 80.550%, known: 82.153%, seen: 62.121%, novel: 64.663%\n",
      "100%|██████████| 500/500 [00:10<00:00, 49.75it/s]\n",
      "100%|██████████| 996/996 [00:06<00:00, 154.58it/s]\n",
      "INFO : Cross-entropy: 0.5691 nats (= perplexity 1.767)\n",
      "100%|██████████| 996/996 [00:04<00:00, 215.34it/s]\n",
      "INFO : Tagging accuracy: all: 80.425%, known: 81.892%, seen: 64.310%, novel: 65.588%\n",
      "100%|██████████| 500/500 [00:09<00:00, 50.76it/s]\n",
      "100%|██████████| 996/996 [00:06<00:00, 154.71it/s]\n",
      "INFO : Cross-entropy: 0.5369 nats (= perplexity 1.711)\n",
      "100%|██████████| 996/996 [00:04<00:00, 204.03it/s]\n",
      "INFO : Tagging accuracy: all: 81.590%, known: 83.192%, seen: 64.141%, novel: 65.324%\n",
      "100%|██████████| 500/500 [00:09<00:00, 50.30it/s]\n",
      "100%|██████████| 996/996 [00:06<00:00, 156.61it/s]\n",
      "INFO : Cross-entropy: 0.5109 nats (= perplexity 1.667)\n",
      "100%|██████████| 996/996 [00:04<00:00, 226.01it/s]\n",
      "INFO : Tagging accuracy: all: 83.603%, known: 85.362%, seen: 63.636%, novel: 66.050%\n",
      "100%|██████████| 500/500 [00:09<00:00, 50.98it/s]\n",
      "100%|██████████| 996/996 [00:06<00:00, 154.26it/s]\n",
      "INFO : Cross-entropy: 0.4887 nats (= perplexity 1.630)\n",
      "100%|██████████| 996/996 [00:04<00:00, 210.84it/s]\n",
      "INFO : Tagging accuracy: all: 84.204%, known: 86.095%, seen: 63.131%, novel: 65.192%\n",
      "100%|██████████| 500/500 [00:09<00:00, 52.13it/s]\n",
      "100%|██████████| 996/996 [00:06<00:00, 160.03it/s]\n",
      "INFO : Cross-entropy: 0.4723 nats (= perplexity 1.604)\n",
      "100%|██████████| 996/996 [00:04<00:00, 218.64it/s]\n",
      "INFO : Tagging accuracy: all: 84.680%, known: 86.612%, seen: 62.963%, novel: 65.324%\n",
      "100%|██████████| 500/500 [00:09<00:00, 52.17it/s]\n",
      "100%|██████████| 996/996 [00:06<00:00, 154.95it/s]\n",
      "INFO : Cross-entropy: 0.4587 nats (= perplexity 1.582)\n",
      "100%|██████████| 996/996 [00:04<00:00, 209.24it/s]\n",
      "INFO : Tagging accuracy: all: 84.880%, known: 86.928%, seen: 62.626%, novel: 64.069%\n",
      "100%|██████████| 500/500 [00:10<00:00, 49.89it/s]\n",
      "100%|██████████| 996/996 [00:06<00:00, 151.67it/s]\n",
      "INFO : Cross-entropy: 0.4441 nats (= perplexity 1.559)\n",
      "100%|██████████| 996/996 [00:04<00:00, 208.98it/s]\n",
      "INFO : Tagging accuracy: all: 85.214%, known: 87.217%, seen: 62.795%, novel: 65.125%\n",
      "100%|██████████| 500/500 [00:10<00:00, 48.32it/s]\n",
      "100%|██████████| 996/996 [00:07<00:00, 139.94it/s]\n",
      "INFO : Cross-entropy: 0.4345 nats (= perplexity 1.544)\n",
      "100%|██████████| 996/996 [00:04<00:00, 216.29it/s]\n",
      "INFO : Tagging accuracy: all: 85.490%, known: 87.436%, seen: 63.636%, novel: 65.984%\n",
      "100%|██████████| 500/500 [00:09<00:00, 51.54it/s]\n",
      "100%|██████████| 996/996 [00:06<00:00, 164.57it/s]\n",
      "INFO : Cross-entropy: 0.4225 nats (= perplexity 1.526)\n",
      "100%|██████████| 996/996 [00:04<00:00, 229.47it/s]\n",
      "INFO : Tagging accuracy: all: 85.891%, known: 87.867%, seen: 63.636%, novel: 66.116%\n",
      "100%|██████████| 500/500 [00:09<00:00, 50.52it/s]\n",
      "100%|██████████| 996/996 [00:06<00:00, 156.32it/s]\n",
      "INFO : Cross-entropy: 0.4162 nats (= perplexity 1.516)\n",
      "100%|██████████| 996/996 [00:04<00:00, 221.97it/s]\n",
      "INFO : Tagging accuracy: all: 85.908%, known: 87.844%, seen: 64.141%, novel: 66.513%\n",
      "  1%|          | 5/500 [00:00<00:09, 49.98it/s]INFO : Saved model to en_crf-7010.pkl\n",
      "100%|██████████| 500/500 [00:09<00:00, 50.24it/s]\n",
      "100%|██████████| 996/996 [00:06<00:00, 157.62it/s]\n",
      "INFO : Cross-entropy: 0.4080 nats (= perplexity 1.504)\n",
      "100%|██████████| 996/996 [00:04<00:00, 226.92it/s]\n",
      "INFO : Tagging accuracy: all: 86.613%, known: 88.563%, seen: 64.646%, novel: 67.107%\n",
      "100%|██████████| 500/500 [00:09<00:00, 54.49it/s]\n",
      "100%|██████████| 996/996 [00:05<00:00, 168.01it/s]\n",
      "INFO : Cross-entropy: 0.3986 nats (= perplexity 1.490)\n",
      "100%|██████████| 996/996 [00:04<00:00, 233.15it/s]\n",
      "INFO : Tagging accuracy: all: 86.283%, known: 88.490%, seen: 62.963%, novel: 63.606%\n",
      "INFO : Saved model to en_crf.pkl\n"
     ]
    }
   ],
   "source": [
    "log.info(\"*** Conditional Random Field (CRF)\\n\")\n",
    "crf = ConditionalRandomField(entrain.tagset, entrain.vocab)  # randomly initialized parameters  \n",
    "crf.train(corpus=ensup, loss=loss_dev, reg=1.0, lr=0.05, minibatch_size=10,\n",
    "          save_path=\"ensup_crf.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine how the CRF does on individual sentences. \n",
    "(Do you see any error patterns here that would inspire additional CRF features?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Gold:    ``/` We/P 're/V strongly/R _OOV_/V that/I anyone/N who/W has/V eaten/V in/I the/D cafeteria/N this/D month/N have/V the/D shot/N ,/, ''/' Mr./N Mattausch/N added/V ,/, ``/` and/C that/D means/V virtually/R everyone/N who/W works/V here/R ./.\n",
      "INFO : Viterbi: ``/` We/P 're/V strongly/J _OOV_/N that/I anyone/N who/W has/V eaten/N in/I the/D cafeteria/N this/D month/N have/V the/D shot/N ,/, ''/' Mr./N Mattausch/N added/N ,/, ``/` and/C that/I means/J virtually/N everyone/N who/W works/V here/R ./.\n",
      "INFO : Loss:    7/34\n",
      "INFO : Cross-entropy: 0.7668604254722595 nats (= perplexity 1.7015628106627378)\n",
      "---\n",
      "INFO : Gold:    I/P was/V _OOV_/V to/T read/V the/D _OOV_/N of/I facts/N in/I your/P Oct./N 13/C editorial/N ``/` _OOV_/N 's/P _OOV_/N _OOV_/N ./. ''/'\n",
      "INFO : Viterbi: I/P was/V _OOV_/V to/T read/V the/D _OOV_/N of/I facts/N in/I your/J Oct./N 13/C editorial/N ``/` _OOV_/N 's/P _OOV_/N _OOV_/N ./. ''/'\n",
      "INFO : Loss:    1/21\n",
      "INFO : Cross-entropy: 0.4758842885494232 nats (= perplexity 1.39077042479066)\n",
      "---\n",
      "INFO : Gold:    It/P is/V the/D _OOV_/J guerrillas/N who/W are/V aligned/V with/I the/D drug/N traffickers/N ,/, not/R the/D left/J _OOV_/N ./.\n",
      "INFO : Viterbi: It/P is/V the/D _OOV_/N guerrillas/N who/W are/V aligned/N with/I the/D drug/N traffickers/N ,/, not/R the/D left/N _OOV_/N ./.\n",
      "INFO : Loss:    3/18\n",
      "INFO : Cross-entropy: 0.3413330018520355 nats (= perplexity 1.266926649259926)\n",
      "---\n",
      "INFO : Gold:    This/D information/N was/V _OOV_/V from/I your/P own/J news/N stories/N on/I the/D region/N ./.\n",
      "INFO : Viterbi: This/D information/N was/V _OOV_/N from/I your/J own/J news/N stories/N on/I the/D region/N ./.\n",
      "INFO : Loss:    2/13\n",
      "INFO : Cross-entropy: 0.45741257071495056 nats (= perplexity 1.3730770482075767)\n",
      "---\n",
      "INFO : Gold:    _OOV_/J _OOV_/J government/N _OOV_/N of/I the/D ``/` _OOV_/F ''/' was/V due/J to/T the/D drug/N _OOV_/N '/P history/N of/I _OOV_/V out/R _OOV_/N in/I the/D _OOV_/N ./.\n",
      "INFO : Viterbi: _OOV_/N _OOV_/N government/N _OOV_/N of/I the/D ``/` _OOV_/N ''/' was/V due/J to/T the/D drug/N _OOV_/V '/P history/N of/I _OOV_/N out/I _OOV_/N in/I the/D _OOV_/N ./.\n",
      "INFO : Loss:    6/25\n",
      "INFO : Cross-entropy: 0.817240297794342 nats (= perplexity 1.762032234470972)\n",
      "---\n",
      "INFO : Gold:    Mary/N _OOV_/N Palo/N Alto/N ,/, Calif/N ./.\n",
      "INFO : Viterbi: Mary/N _OOV_/N Palo/N Alto/N ,/, Calif/N ./.\n",
      "INFO : Loss:    0/7\n",
      "INFO : Cross-entropy: 0.4366017282009125 nats (= perplexity 1.3534126102183663)\n",
      "---\n",
      "INFO : Gold:    I/P suggest/V that/I The/D Wall/N Street/N Journal/N -LRB-/- as/R well/R as/I other/J U.S./N news/N publications/N of/I like/J mind/N -RRB-/- should/M put/V its/P money/N where/W its/P mouth/N is/V :/: _OOV_/V computer/N equipment/N to/T replace/V that/I damaged/V at/I El/N _OOV_/N ,/, buy/V ad/N space/N ,/, publish/V stories/N under/I the/D _OOV_/N of/I El/N _OOV_/N journalists/N ./.\n",
      "INFO : Viterbi: I/P suggest/N that/I The/D Wall/N Street/N Journal/N -LRB-/- as/I well/R as/I other/J U.S./N news/N publications/N of/I like/J mind/N -RRB-/- should/M put/V its/P money/N where/W its/P mouth/N is/V :/: _OOV_/N computer/N equipment/N to/T replace/V that/D damaged/N at/I El/N _OOV_/N ,/, buy/V ad/N space/N ,/, publish/N stories/N under/I the/D _OOV_/N of/I El/N _OOV_/N journalists/N ./.\n",
      "INFO : Loss:    6/53\n",
      "INFO : Cross-entropy: 0.5870106220245361 nats (= perplexity 1.502130989608012)\n",
      "---\n",
      "INFO : Gold:    Perhaps/R an/D arrangement/N could/M be/V worked/V out/R to/T ``/` sponsor/V ''/' El/N _OOV_/N journalists/N and/C staff/N by/I paying/V for/I added/V security/N in/I exchange/N for/I exclusive/J stories/N ./.\n",
      "INFO : Viterbi: Perhaps/I an/D arrangement/N could/M be/V worked/V out/R to/T ``/` sponsor/N ''/' El/N _OOV_/N journalists/N and/C staff/N by/I paying/N for/I added/J security/N in/I exchange/N for/I exclusive/J stories/N ./.\n",
      "INFO : Loss:    4/27\n",
      "INFO : Cross-entropy: 0.7428496479988098 nats (= perplexity 1.6734780768866748)\n",
      "---\n",
      "INFO : Gold:    _OOV_/V El/N _OOV_/N 's/P courage/N with/I real/J support/N ./.\n",
      "INFO : Viterbi: _OOV_/N El/N _OOV_/V 's/P courage/N with/I real/J support/N ./.\n",
      "INFO : Loss:    2/9\n",
      "INFO : Cross-entropy: 0.8443189859390259 nats (= perplexity 1.7954170310609199)\n",
      "---\n",
      "INFO : Gold:    Douglas/N B./N Evans/N\n",
      "INFO : Viterbi: Douglas/N B./N Evans/.\n",
      "INFO : Loss:    1/3\n",
      "INFO : Cross-entropy: 0.7590414881706238 nats (= perplexity 1.6923658439131315)\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "look_at_your_data(crf, endev, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO : Loaded model from en_crf.pkl\n",
      "100%|██████████| 996/996 [00:10<00:00, 98.56it/s] \n",
      "INFO : Cross-entropy: 0.3986 nats (= perplexity 1.490)\n",
      "100%|██████████| 996/996 [00:05<00:00, 182.65it/s]\n",
      "INFO : Tagging accuracy: all: 86.283%, known: 88.490%, seen: 62.963%, novel: 63.606%\n",
      "100%|██████████| 500/500 [00:10<00:00, 46.07it/s]\n",
      "100%|██████████| 996/996 [00:10<00:00, 94.32it/s] \n",
      "INFO : Cross-entropy: 0.3972 nats (= perplexity 1.488)\n",
      "100%|██████████| 996/996 [00:05<00:00, 181.47it/s]\n",
      "INFO : Tagging accuracy: all: 86.350%, known: 88.284%, seen: 64.983%, novel: 66.843%\n",
      "100%|██████████| 500/500 [00:10<00:00, 49.15it/s]\n",
      "100%|██████████| 996/996 [00:10<00:00, 97.72it/s] \n",
      "INFO : Cross-entropy: 0.3918 nats (= perplexity 1.480)\n",
      "100%|██████████| 996/996 [00:05<00:00, 183.46it/s]\n",
      "INFO : Tagging accuracy: all: 86.964%, known: 89.103%, seen: 63.468%, novel: 65.324%\n",
      "100%|██████████| 500/500 [00:10<00:00, 49.68it/s]\n",
      "100%|██████████| 996/996 [00:09<00:00, 103.99it/s]\n",
      "INFO : Cross-entropy: 0.3867 nats (= perplexity 1.472)\n",
      "100%|██████████| 996/996 [00:05<00:00, 187.30it/s]\n",
      "INFO : Tagging accuracy: all: 87.181%, known: 89.277%, seen: 63.300%, novel: 66.314%\n",
      "100%|██████████| 500/500 [00:10<00:00, 49.54it/s]\n",
      "100%|██████████| 996/996 [00:10<00:00, 98.11it/s] \n",
      "INFO : Cross-entropy: 0.3872 nats (= perplexity 1.473)\n",
      "100%|██████████| 996/996 [00:05<00:00, 177.95it/s]\n",
      "INFO : Tagging accuracy: all: 87.586%, known: 89.927%, seen: 62.626%, novel: 63.606%\n",
      "100%|██████████| 500/500 [00:10<00:00, 48.55it/s]\n",
      "100%|██████████| 996/996 [00:10<00:00, 96.36it/s] \n",
      "INFO : Cross-entropy: 0.3813 nats (= perplexity 1.464)\n",
      "100%|██████████| 996/996 [00:05<00:00, 176.45it/s]\n",
      "INFO : Tagging accuracy: all: 87.148%, known: 89.350%, seen: 62.963%, novel: 64.861%\n",
      "100%|██████████| 500/500 [00:10<00:00, 47.26it/s]\n",
      "100%|██████████| 996/996 [00:10<00:00, 98.65it/s] \n",
      "INFO : Cross-entropy: 0.3802 nats (= perplexity 1.463)\n",
      "100%|██████████| 996/996 [00:05<00:00, 188.46it/s]\n",
      "INFO : Tagging accuracy: all: 86.651%, known: 88.700%, seen: 63.300%, novel: 66.248%\n",
      "100%|██████████| 500/500 [00:10<00:00, 48.17it/s]\n",
      "100%|██████████| 996/996 [00:10<00:00, 95.31it/s] \n",
      "INFO : Cross-entropy: 0.3746 nats (= perplexity 1.454)\n",
      "100%|██████████| 996/996 [00:05<00:00, 180.37it/s]\n",
      "INFO : Tagging accuracy: all: 87.135%, known: 89.254%, seen: 63.973%, novel: 65.654%\n",
      "100%|██████████| 500/500 [00:10<00:00, 47.94it/s]\n",
      "100%|██████████| 996/996 [00:10<00:00, 96.87it/s] \n",
      "INFO : Cross-entropy: 0.3730 nats (= perplexity 1.452)\n",
      "100%|██████████| 996/996 [00:05<00:00, 188.68it/s]\n",
      "INFO : Tagging accuracy: all: 88.037%, known: 90.316%, seen: 62.795%, novel: 65.059%\n",
      "100%|██████████| 500/500 [00:10<00:00, 47.94it/s]\n",
      "100%|██████████| 996/996 [00:10<00:00, 97.08it/s] \n",
      "INFO : Cross-entropy: 0.3702 nats (= perplexity 1.448)\n",
      "100%|██████████| 996/996 [00:05<00:00, 192.36it/s]\n",
      "INFO : Tagging accuracy: all: 86.847%, known: 88.947%, seen: 63.973%, novel: 65.522%\n",
      "100%|██████████| 500/500 [00:10<00:00, 49.53it/s]\n",
      "100%|██████████| 996/996 [00:10<00:00, 98.37it/s] \n",
      "INFO : Cross-entropy: 0.3677 nats (= perplexity 1.444)\n",
      "100%|██████████| 996/996 [00:05<00:00, 183.34it/s]\n",
      "INFO : Tagging accuracy: all: 87.812%, known: 90.051%, seen: 63.300%, novel: 65.125%\n",
      "100%|██████████| 500/500 [00:10<00:00, 46.48it/s]\n",
      "100%|██████████| 996/996 [00:10<00:00, 98.86it/s] \n",
      "INFO : Cross-entropy: 0.3678 nats (= perplexity 1.445)\n",
      "100%|██████████| 996/996 [00:05<00:00, 185.92it/s]\n",
      "INFO : Tagging accuracy: all: 86.976%, known: 89.121%, seen: 63.131%, novel: 65.390%\n",
      "  1%|          | 5/500 [00:00<00:12, 40.65it/s]INFO : Saved model to en_crf_raw-5510.pkl\n",
      "100%|██████████| 500/500 [00:09<00:00, 50.62it/s]\n",
      "100%|██████████| 996/996 [00:09<00:00, 102.94it/s]\n",
      "INFO : Cross-entropy: 0.3636 nats (= perplexity 1.438)\n",
      "100%|██████████| 996/996 [00:05<00:00, 182.23it/s]\n",
      "INFO : Tagging accuracy: all: 86.997%, known: 89.149%, seen: 62.458%, novel: 65.588%\n",
      "100%|██████████| 500/500 [00:10<00:00, 48.94it/s]\n",
      "100%|██████████| 996/996 [00:10<00:00, 98.52it/s] \n",
      "INFO : Cross-entropy: 0.3627 nats (= perplexity 1.437)\n",
      "100%|██████████| 996/996 [00:05<00:00, 190.41it/s]\n",
      "INFO : Tagging accuracy: all: 87.156%, known: 89.300%, seen: 63.300%, novel: 65.588%\n",
      "100%|██████████| 500/500 [00:10<00:00, 48.15it/s]\n",
      "100%|██████████| 996/996 [00:10<00:00, 96.97it/s] \n",
      "INFO : Cross-entropy: 0.3582 nats (= perplexity 1.431)\n",
      "100%|██████████| 996/996 [00:05<00:00, 189.72it/s]\n",
      "INFO : Tagging accuracy: all: 88.221%, known: 90.413%, seen: 63.131%, novel: 66.446%\n",
      "100%|██████████| 500/500 [00:10<00:00, 49.03it/s]\n",
      "100%|██████████| 996/996 [00:10<00:00, 98.36it/s] \n",
      "INFO : Cross-entropy: 0.3585 nats (= perplexity 1.431)\n",
      "100%|██████████| 996/996 [00:05<00:00, 173.55it/s]\n",
      "INFO : Tagging accuracy: all: 88.079%, known: 90.216%, seen: 63.300%, novel: 66.975%\n",
      "100%|██████████| 500/500 [00:10<00:00, 48.51it/s]\n",
      "100%|██████████| 996/996 [00:09<00:00, 101.83it/s]\n",
      "INFO : Cross-entropy: 0.3552 nats (= perplexity 1.426)\n",
      "100%|██████████| 996/996 [00:04<00:00, 199.96it/s]\n",
      "INFO : Tagging accuracy: all: 87.937%, known: 90.225%, seen: 62.458%, novel: 64.927%\n",
      "100%|██████████| 500/500 [00:09<00:00, 51.40it/s]\n",
      "100%|██████████| 996/996 [00:09<00:00, 104.36it/s]\n",
      "INFO : Cross-entropy: 0.3518 nats (= perplexity 1.422)\n",
      "100%|██████████| 996/996 [00:04<00:00, 204.20it/s]\n",
      "INFO : Tagging accuracy: all: 88.229%, known: 90.509%, seen: 62.963%, novel: 65.258%\n",
      "100%|██████████| 500/500 [00:10<00:00, 48.94it/s]\n",
      "100%|██████████| 996/996 [00:09<00:00, 100.54it/s]\n",
      "INFO : Cross-entropy: 0.3503 nats (= perplexity 1.420)\n",
      "100%|██████████| 996/996 [00:05<00:00, 194.20it/s]\n",
      "INFO : Tagging accuracy: all: 88.317%, known: 90.619%, seen: 62.458%, novel: 65.258%\n",
      "100%|██████████| 500/500 [00:10<00:00, 49.56it/s]\n",
      "100%|██████████| 996/996 [00:09<00:00, 100.75it/s]\n",
      "INFO : Cross-entropy: 0.3510 nats (= perplexity 1.420)\n",
      "100%|██████████| 996/996 [00:05<00:00, 191.82it/s]\n",
      "INFO : Tagging accuracy: all: 88.288%, known: 90.408%, seen: 63.636%, novel: 67.371%\n",
      "INFO : Saved model to en_crf_raw.pkl\n"
     ]
    }
   ],
   "source": [
    "hmm = ConditionalRandomField.load(\"en_crf.pkl\")  # reset to supervised model (in case you're re-executing this bit)\n",
    "loss_dev = lambda model: viterbi_error_rate(model, eval_corpus=endev, \n",
    "                                            known_vocab=known_vocab)\n",
    "hmm.train(corpus=entrain, loss=loss_dev, reg=1.0, lr=0.05, minibatch_size=10,\n",
    "          save_path=\"en_crf_raw.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Conda环境",
   "language": "python",
   "name": "conda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
